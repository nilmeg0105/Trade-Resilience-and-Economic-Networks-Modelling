{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nilmeg0105/Trade-Resilience-and-Economic-Networks-Modelling/blob/main/DPL_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Hid2iCIrGO4",
        "outputId": "4c16fbfd-af74-4407-91a7-011a037714f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.5.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.16.4-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.43)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.14.1)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.4)\n",
            "Downloading optuna-4.5.0-py3-none-any.whl (400 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.9/400.9 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.16.4-py3-none-any.whl (247 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.0/247.0 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, alembic, optuna\n",
            "Successfully installed alembic-1.16.4 colorlog-6.9.0 optuna-4.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "import optuna\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "export_df = pd.read_csv('/content/export_processed.csv', encoding=\"latin1\")  # Replace with actual path\n",
        "master_df = pd.read_csv('/content/integrated_master.csv', encoding=\"latin1\")  # Replace with actual path\n",
        "resilience_df = pd.read_csv('/content/processed_resiliance.csv', encoding=\"latin1\")  # Replace with actual path\n",
        "# 2. Preprocess and Merge Datasets\n",
        "# Rename columns for consistency\n",
        "export_df = export_df.rename(columns={'refYear': 'Year', 'reporterISO': 'Country Code'})\n",
        "resilience_df = resilience_df[resilience_df['Series Name'] == 'Current account balance (% of GDP)']\n",
        "\n",
        "# Pivot resilience dataset\n",
        "resilience_df = resilience_df.pivot(index=['Country Code', 'Year'],\n",
        "                                   columns='Series Name',\n",
        "                                   values='Value').reset_index()\n",
        "resilience_df.columns = ['Country Code', 'Year', 'Current account balance (% of GDP)']\n",
        "\n",
        "# Merge datasets\n",
        "merged_df = export_df.merge(master_df, on=['Country Code', 'Year'], how='left')\n",
        "merged_df = merged_df.merge(resilience_df, on=['Country Code', 'Year'], how='left')\n",
        "\n",
        "# 3. Feature Selection\n",
        "features = [\n",
        "    'TradeDependencyIndex', 'TotalPrimaryValue', 'TotalFOBValue',\n",
        "    'HHI_export', 'HHI_import', 'Diversification_export', 'Diversification_import',\n",
        "    'Trade_Diversification_Index', 'trade_gdp_pct', 'gdp_growth_pct',\n",
        "    'gdp_per_capita_current_usd', 'Current account balance (% of GDP)'\n",
        "]\n",
        "\n",
        "# Check available features\n",
        "available_features = [col for col in features if col in merged_df.columns]\n",
        "if not available_features:\n",
        "    raise ValueError(\"No valid features available for modeling.\")\n",
        "print(f\"Selected features: {available_features}\")\n",
        "\n",
        "# Subset dataframe\n",
        "data = merged_df[['Country Code', 'partnerISO', 'Year'] + available_features].copy()\n",
        "\n",
        "# 4. Handle Missing Data\n",
        "# Forward-fill for time-series data\n",
        "data = data.sort_values(['Country Code', 'Year'])\n",
        "data['Current account balance (% of GDP)'] = data.groupby('Country Code')['Current account balance (% of GDP)'].fillna(method='ffill')\n",
        "# Impute remaining missing values with median\n",
        "data[available_features] = data[available_features].fillna(data[available_features].median())\n",
        "\n",
        "# 5. Normalize Features\n",
        "scaler = MinMaxScaler()\n",
        "data[available_features] = scaler.fit_transform(data[available_features])\n",
        "\n",
        "# 6. Create Time-Series Sequences\n",
        "def create_sequences(data, seq_length=5):\n",
        "    sequences = []\n",
        "    targets = []\n",
        "    countries = data['Country Code'].unique()\n",
        "\n",
        "    for country in countries:\n",
        "        country_data = data[data['Country Code'] == country].sort_values('Year')\n",
        "        if len(country_data) >= seq_length + 1:  # Ensure enough data for sequence + target\n",
        "            for i in range(len(country_data) - seq_length):\n",
        "                seq = country_data[available_features].iloc[i:i+seq_length].values\n",
        "                target = country_data['Trade_Diversification_Index'].iloc[i+seq_length]\n",
        "                if not np.isnan(target):  # Skip if target is missing\n",
        "                    sequences.append(seq)\n",
        "                    targets.append(target)\n",
        "\n",
        "    sequences = np.array(sequences)\n",
        "    targets = np.array(targets)\n",
        "    print(f\"Generated sequences shape: {sequences.shape}, targets shape: {targets.shape}\")\n",
        "    return sequences, targets\n",
        "\n",
        "seq_length = 5\n",
        "X, y = create_sequences(data, seq_length)\n",
        "\n",
        "# Validate sequences\n",
        "if X.size == 0 or y.size == 0:\n",
        "    raise ValueError(\"No valid sequences generated. Check data or reduce seq_length.\")\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 7. Define LSTM Model with Optuna\n",
        "def build_model(trial):\n",
        "    n_units = trial.suggest_int('n_units', 32, 128)\n",
        "    n_layers = trial.suggest_int('n_layers', 1, 3)\n",
        "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
        "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
        "\n",
        "    model = Sequential()\n",
        "    # First LSTM layer\n",
        "    model.add(LSTM(units=n_units, return_sequences=(n_layers > 1),\n",
        "                   input_shape=(seq_length, len(available_features))))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "\n",
        "    # Additional LSTM layers\n",
        "    for i in range(n_layers - 1):\n",
        "        model.add(LSTM(units=n_units, return_sequences=(i < n_layers - 2)))\n",
        "        model.add(Dropout(dropout_rate))\n",
        "\n",
        "    # Dense output layer\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "                  loss='mse')\n",
        "    return model\n",
        "\n",
        "# 8. Objective Function for Optuna\n",
        "def objective(trial):\n",
        "    model = build_model(trial)\n",
        "    model.fit(X_train, y_train, epochs=10, batch_size=32,\n",
        "              validation_split=0.2, verbose=0)\n",
        "    loss = model.evaluate(X_test, y_test, verbose=0)\n",
        "    return loss\n",
        "\n",
        "# 9. Run Optuna Optimization\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=20)\n",
        "\n",
        "# Get best model\n",
        "best_trial = study.best_trial\n",
        "print(f\"Best hyperparameters: {best_trial.params}\")\n",
        "\n",
        "# Build and train final model\n",
        "final_model = build_model(optuna.trial.FixedTrial(best_trial.params))\n",
        "final_model.fit(X_train, y_train, epochs=20, batch_size=32,\n",
        "                validation_split=0.2, verbose=1)\n",
        "\n",
        "# 10. Generate Recommendations\n",
        "def recommend_trade_partners(data, model, scaler, seq_length=5, top_n=3):\n",
        "    recommendations = {}\n",
        "    countries = data['Country Code'].unique()\n",
        "\n",
        "    for country in countries:\n",
        "        country_data = data[data['Country Code'] == country].sort_values('Year')\n",
        "        current_partners = country_data['partnerISO'].unique()\n",
        "\n",
        "        # Get potential partners (exclude current partners and self)\n",
        "        all_countries = data['Country Code'].unique()\n",
        "        potential_partners = [p for p in all_countries if p not in current_partners and p != country]\n",
        "\n",
        "        partner_scores = []\n",
        "        for partner in potential_partners:\n",
        "            partner_data = data[data['Country Code'] == partner].sort_values('Year')\n",
        "            if len(partner_data) >= seq_length:\n",
        "                seq = partner_data[available_features].iloc[-seq_length:].values\n",
        "                seq = np.expand_dims(seq, axis=0)\n",
        "                pred = model.predict(seq, verbose=0)[0][0]\n",
        "                # Adjust score: favor high diversification, low dependency\n",
        "                partner_trade_data = data[data['Country Code'] == partner]\n",
        "                avg_dependency = partner_trade_data['TradeDependencyIndex'].mean() if 'TradeDependencyIndex' in partner_trade_data else 0\n",
        "                score = pred - avg_dependency\n",
        "                partner_scores.append((partner, score))\n",
        "\n",
        "        # Sort by score (higher is better)\n",
        "        partner_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "        recommendations[country] = [p[0] for p in partner_scores[:top_n]]\n",
        "\n",
        "    return recommendations\n",
        "\n",
        "# Generate recommendations\n",
        "recommendations = recommend_trade_partners(data, final_model, scaler)\n",
        "\n",
        "# 11. Output Recommendations\n",
        "for country, partners in recommendations.items():\n",
        "    print(f\"Recommended trade partners for {country}: {partners}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "3xs5m88Lx9OK",
        "outputId": "b6f21a14-2fd3-44c0-8d27-f1b628f123cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected features: ['TradeDependencyIndex', 'TotalPrimaryValue', 'TotalFOBValue', 'HHI_export', 'HHI_import', 'Diversification_export', 'Diversification_import', 'Trade_Diversification_Index', 'trade_gdp_pct', 'gdp_growth_pct', 'gdp_per_capita_current_usd', 'Current account balance (% of GDP)']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-08-19 16:20:21,875] A new study created in memory with name: no-name-ffab57b6-259d-4d3e-ae1a-4cac84aab901\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated sequences shape: (82809, 5, 12), targets shape: (82809,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-08-19 16:22:02,002] Trial 0 finished with value: 3.758849197765812e-05 and parameters: {'n_units': 37, 'n_layers': 1, 'dropout_rate': 0.41680430411626623, 'learning_rate': 0.008081349295225157}. Best is trial 0 with value: 3.758849197765812e-05.\n",
            "[W 2025-08-19 16:22:46,214] Trial 1 failed with parameters: {'n_units': 110, 'n_layers': 2, 'dropout_rate': 0.13159880803402446, 'learning_rate': 0.001393399719076829} because of the following error: KeyboardInterrupt().\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 201, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "                      ^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-3443970249.py\", line 116, in objective\n",
            "    model.fit(X_train, y_train, epochs=10, batch_size=32,\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 401, in fit\n",
            "    val_logs = self.evaluate(\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 489, in evaluate\n",
            "    logs = self.test_function(iterator)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 220, in function\n",
            "    opt_outputs = multi_step_on_iterator(iterator)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\", line 150, in error_handler\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 833, in __call__\n",
            "    result = self._call(*args, **kwds)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 878, in _call\n",
            "    results = tracing_compilation.call_function(\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\", line 139, in call_function\n",
            "    return function._call_flat(  # pylint: disable=protected-access\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\", line 1322, in _call_flat\n",
            "    return self._inference_function.call_preflattened(args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\", line 216, in call_preflattened\n",
            "    flat_outputs = self.call_flat(*args)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\", line 251, in call_flat\n",
            "    outputs = self._bound_context.call_function(\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/context.py\", line 1688, in call_function\n",
            "    outputs = execute.execute(\n",
            "              ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\", line 53, in quick_execute\n",
            "    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "[W 2025-08-19 16:22:46,217] Trial 1 failed with value None.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3443970249.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;31m# 9. Run Optuna Optimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'minimize'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;31m# Get best model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    488\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         \"\"\"\n\u001b[0;32m--> 490\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    491\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     64\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mfrozen_trial_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     ):\n\u001b[0;32m--> 258\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3443970249.py\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mobjective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m     model.fit(X_train, y_train, epochs=10, batch_size=32, \n\u001b[0m\u001b[1;32m    117\u001b[0m               validation_split=0.2, verbose=0)\n\u001b[1;32m    118\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    399\u001b[0m                         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m                     )\n\u001b[0;32m--> 401\u001b[0;31m                 val_logs = self.evaluate(\n\u001b[0m\u001b[1;32m    402\u001b[0m                     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m                     \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m                 \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_evaluating\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             ):\n\u001b[0;32m--> 220\u001b[0;31m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1686\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1688\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1689\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# 1. Load Datasets\n",
        "export_df = pd.read_csv('/content/export_processed.csv', encoding=\"latin1\")\n",
        "master_df = pd.read_csv('/content/integrated_master.csv', encoding=\"latin1\")\n",
        "resilience_df = pd.read_csv('/content/processed_resiliance.csv', encoding=\"latin1\")\n",
        "\n",
        "# 2. Preprocess and Merge Datasets\n",
        "# Rename columns for consistency\n",
        "export_df = export_df.rename(columns={'refYear': 'Year', 'reporterISO': 'Country Code'})\n",
        "resilience_df = resilience_df[resilience_df['Series Name'] == 'Current account balance (% of GDP)']\n",
        "\n",
        "# Pivot resilience dataset\n",
        "resilience_df = resilience_df.pivot(index=['Country Code', 'Year'],\n",
        "                                   columns='Series Name',\n",
        "                                   values='Value').reset_index()\n",
        "resilience_df.columns = ['Country Code', 'Year', 'Current account balance (% of GDP)']\n",
        "\n",
        "# Merge datasets\n",
        "merged_df = export_df.merge(master_df, on=['Country Code', 'Year'], how='left')\n",
        "merged_df = merged_df.merge(resilience_df, on=['Country Code', 'Year'], how='left')\n",
        "\n",
        "# 3. Feature Selection\n",
        "features = [\n",
        "    'TradeDependencyIndex', 'TotalPrimaryValue', 'TotalFOBValue',\n",
        "    'HHI_export', 'HHI_import', 'Diversification_export', 'Diversification_import',\n",
        "    'Trade_Diversification_Index', 'trade_gdp_pct', 'gdp_growth_pct',\n",
        "    'gdp_per_capita_current_usd', 'Current account balance (% of GDP)'\n",
        "]\n",
        "\n",
        "# Check available features\n",
        "available_features = [col for col in features if col in merged_df.columns]\n",
        "if not available_features:\n",
        "    raise ValueError(\"No valid features available for modeling.\")\n",
        "print(f\"Selected features: {available_features}\")\n",
        "\n",
        "# Subset dataframe\n",
        "data = merged_df[['Country Code', 'partnerISO', 'Year'] + available_features].copy()\n",
        "\n",
        "# 4. Handle Missing Data\n",
        "# Forward-fill for time-series data\n",
        "data = data.sort_values(['Country Code', 'Year'])\n",
        "data['Current account balance (% of GDP)'] = data.groupby('Country Code')['Current account balance (% of GDP)'].fillna(method='ffill')\n",
        "# Impute remaining missing values with median\n",
        "data[available_features] = data[available_features].fillna(data[available_features].median())\n",
        "\n",
        "# 5. Normalize Features\n",
        "scaler = MinMaxScaler()\n",
        "data[available_features] = scaler.fit_transform(data[available_features])\n",
        "\n",
        "# 6. Create Flattened Sequences for Random Forest\n",
        "def create_flattened_sequences(data, seq_length=5):\n",
        "    sequences = []\n",
        "    targets = []\n",
        "    feature_names = []\n",
        "    countries = data['Country Code'].unique()\n",
        "\n",
        "    # Create feature names for lagged variables\n",
        "    for feature in available_features:\n",
        "        for lag in range(1, seq_length + 1):\n",
        "            feature_names.append(f\"{feature}_t-{lag}\")\n",
        "\n",
        "    for country in countries:\n",
        "        country_data = data[data['Country Code'] == country].sort_values('Year')\n",
        "        if len(country_data) >= seq_length + 1:\n",
        "            for i in range(len(country_data) - seq_length):\n",
        "                seq = country_data[available_features].iloc[i:i+seq_length].values\n",
        "                # Flatten the sequence: shape (seq_length, n_features) -> (seq_length * n_features,)\n",
        "                flattened_seq = seq.flatten()\n",
        "                target = country_data['Trade_Diversification_Index'].iloc[i+seq_length]\n",
        "                if not np.isnan(target):\n",
        "                    sequences.append(flattened_seq)\n",
        "                    targets.append(target)\n",
        "\n",
        "    sequences = np.array(sequences)\n",
        "    targets = np.array(targets)\n",
        "    print(f\"Generated sequences shape: {sequences.shape}, targets shape: {targets.shape}\")\n",
        "    return sequences, targets, feature_names\n",
        "\n",
        "seq_length = 5\n",
        "X, y, feature_names = create_flattened_sequences(data, seq_length)\n",
        "\n",
        "# Validate sequences\n",
        "if X.size == 0 or y.size == 0:\n",
        "    raise ValueError(\"No valid sequences generated. Check data or reduce seq_length.\")\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 7. Train Random Forest Model\n",
        "model = RandomForestRegressor(\n",
        "    n_estimators=100,\n",
        "    max_depth=20,\n",
        "    min_samples_split=5,\n",
        "    min_samples_leaf=2,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Print model performance\n",
        "print(f\"Random Forest R^2 score on test set: {model.score(X_test, y_test)}\")\n",
        "\n",
        "# 8. Generate Recommendations\n",
        "def recommend_trade_partners(data, model, scaler, seq_length=5, top_n=3):\n",
        "    recommendations = {}\n",
        "    countries = data['Country Code'].unique()\n",
        "\n",
        "    for country in countries:\n",
        "        country_data = data[data['Country Code'] == country].sort_values('Year')\n",
        "        current_partners = country_data['partnerISO'].unique()\n",
        "\n",
        "        # Get potential partners (exclude current partners and self)\n",
        "        all_countries = data['Country Code'].unique()\n",
        "        potential_partners = [p for p in all_countries if p not in current_partners and p != country]\n",
        "\n",
        "        partner_scores = []\n",
        "        for partner in potential_partners:\n",
        "            partner_data = data[data['Country Code'] == partner].sort_values('Year')\n",
        "            if len(partner_data) >= seq_length:\n",
        "                seq = partner_data[available_features].iloc[-seq_length:].values\n",
        "                flattened_seq = seq.flatten()\n",
        "                flattened_seq = np.expand_dims(flattened_seq, axis=0)\n",
        "                pred = model.predict(flattened_seq)[0]\n",
        "                # Adjust score: favor high diversification, low dependency\n",
        "                partner_trade_data = data[data['Country Code'] == partner]\n",
        "                avg_dependency = partner_trade_data['TradeDependencyIndex'].mean() if 'TradeDependencyIndex' in partner_trade_data else 0\n",
        "                score = pred - avg_dependency\n",
        "                partner_scores.append((partner, score))\n",
        "\n",
        "        # Sort by score (higher is better)\n",
        "        partner_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "        recommendations[country] = [p[0] for p in partner_scores[:top_n]]\n",
        "\n",
        "    return recommendations\n",
        "\n",
        "# Generate recommendations\n",
        "recommendations = recommend_trade_partners(data, model, scaler)\n",
        "\n",
        "# 9. Output Recommendations\n",
        "for country, partners in recommendations.items():\n",
        "    print(f\"Recommended trade partners for {country}: {partners}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bag2EKkRHLGp",
        "outputId": "a981cd0b-9838-4031-9f89-0293488d035e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected features: ['TradeDependencyIndex', 'TotalPrimaryValue', 'TotalFOBValue', 'HHI_export', 'HHI_import', 'Diversification_export', 'Diversification_import', 'Trade_Diversification_Index', 'trade_gdp_pct', 'gdp_growth_pct', 'gdp_per_capita_current_usd', 'Current account balance (% of GDP)']\n",
            "Generated sequences shape: (82809, 60), targets shape: (82809,)\n",
            "Random Forest R^2 score on test set: 0.9980789563860485\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings, math\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "import optuna\n",
        "\n",
        "SEQ_LEN = 5\n",
        "N_TRIALS = 20\n",
        "EPOCHS_TUNE = 10\n",
        "EPOCHS_FINAL = 25\n",
        "BATCH_SIZE = 32\n",
        "RANDOM_STATE = 42\n",
        "TARGETS = {\n",
        "    \"gdp_current_usd\": \"GDP (current US$)\",\n",
        "    \"poverty_rate_pct\": \"Poverty rate (%)\",\n",
        "    \"unemployment_total_pct\": \"Unemployment rate (%)\"\n",
        "}\n",
        "PREFERRED_FEATURES = [\n",
        "    'Trade_Diversification_Index', 'HHI_export', 'HHI_import',\n",
        "    'Diversification_export', 'Diversification_import', 'Overall_Trade_Dependency',\n",
        "    'trade_gdp_pct', 'imports_goods_services_gdp_pct', 'exports_goods_services_gdp_pct',\n",
        "    'gdp_growth_pct', 'gdp_per_capita_current_usd', 'inflation_consumer_prices_pct',\n",
        "    'log_gdp_current_usd', 'log_gdp_per_capita', 'economic_shock_sensitivity',\n",
        "    'TradeDependencyIndex_mean', 'TradeDependencyIndex_max',\n",
        "    'CountryTotalPrimaryValue', 'CountryTotalFOBValue',\n",
        "    'Current account balance (% of GDP)',\n",
        "    'disaster_severity'\n",
        "]\n",
        "export_df = pd.read_csv('/content/export_processed.csv', encoding='latin1')\n",
        "master_df = pd.read_csv('/content/integrated_master.csv', encoding='latin1')\n",
        "res_df    = pd.read_csv('/content/processed_resiliance.csv', encoding='latin1')\n",
        "\n",
        "\n",
        "#columns in export_df: reporterISO, refYear, TradeDependencyIndex, CountryTotalPrimaryValue, CountryTotalFOBValue\n",
        "#aggregate to get country-year level signals\n",
        "if 'reporterISO' in export_df.columns and 'refYear' in export_df.columns:\n",
        "    exp = export_df.rename(columns={'reporterISO':'Country Code', 'refYear':'Year'})\n",
        "else:\n",
        "    raise ValueError(\"export_processed.csv must have 'reporterISO' and 'refYear'\")\n",
        "\n",
        "agg = {\n",
        "    'TradeDependencyIndex': ['mean','max'],\n",
        "}\n",
        "for c in ['CountryTotalPrimaryValue','CountryTotalFOBValue']:\n",
        "    if c in export_df.columns:\n",
        "        agg[c] = 'sum'\n",
        "\n",
        "export_country_year = export_df.groupby(['reporterISO','refYear']).agg(agg)\n",
        "export_country_year.columns = ['_'.join([a,b]) if isinstance(b,str) else a for a,b in export_country_year.columns.ravel()]\n",
        "export_country_year = export_country_year.reset_index().rename(columns={\n",
        "    'reporterISO':'Country Code','refYear':'Year',\n",
        "    'TradeDependencyIndex_mean':'TradeDependencyIndex_mean',\n",
        "    'TradeDependencyIndex_max':'TradeDependencyIndex_max'\n",
        "})\n",
        "\n",
        "desired_series = ['Current account balance (% of GDP)']\n",
        "res_keep = res_df[res_df['Series Name'].isin(desired_series)].copy()\n",
        "\n",
        "if {'Country Code','Year','Series Name','Value'}.issubset(res_keep.columns):\n",
        "    res_pivot = res_keep.pivot_table(index=['Country Code','Year'],\n",
        "                                     columns='Series Name',\n",
        "                                     values='Value',\n",
        "                                     aggfunc='mean').reset_index()\n",
        "else:\n",
        "    res_pivot = pd.DataFrame(columns=['Country Code','Year'] + desired_series)\n",
        "if not {'Country Code','Year'}.issubset(master_df.columns):\n",
        "    if {'ISO','Year'}.issubset(master_df.columns):\n",
        "        master_df = master_df.rename(columns={'ISO':'Country Code'})\n",
        "    else:\n",
        "        raise ValueError(\"integrated_master.csv must have 'Country Code' and 'Year' (or 'ISO' + 'Year').\")\n",
        "for col in ['poverty_rate_pct','unemployment_total_pct']:\n",
        "    if col not in master_df.columns:\n",
        "        master_df[col] = np.nan\n",
        "if 'disaster_severity' not in master_df.columns:\n",
        "    master_df['disaster_severity'] = 0.0\n",
        "\n",
        "#Merging logic\n",
        "df = master_df.merge(export_country_year, on=['Country Code','Year'], how='left') \\\n",
        "              .merge(res_pivot,          on=['Country Code','Year'], how='left')\n",
        "\n",
        "feature_cols = [c for c in PREFERRED_FEATURES if c in df.columns]\n",
        "if len(feature_cols) == 0:\n",
        "    raise ValueError(\"No usable features.\")\n",
        "df = df.sort_values(['Country Code','Year']).reset_index(drop=True)\n",
        "\n",
        "# Forward-fill time features by country for stability (As explained in the README)\n",
        "for col in feature_cols:\n",
        "    df[col] = df.groupby('Country Code')[col].ffill()\n",
        "for col in feature_cols:\n",
        "    if df[col].isna().any():\n",
        "        # fill per-country median, then global median\n",
        "        df[col] = df.groupby('Country Code')[col].apply(lambda s: s.fillna(s.median()))\n",
        "        df[col] = df[col].fillna(df[col].median())\n",
        "\n",
        "#Basic helper functions\n",
        "def build_sequences(panel, seq_len, feature_cols, target_col):\n",
        "    X_list, y_list = [], []\n",
        "    for cc, cdf in panel.groupby('Country Code'):\n",
        "        cdf = cdf.sort_values('Year')\n",
        "        if target_col not in cdf.columns:\n",
        "            continue\n",
        "        vals = cdf[feature_cols].values\n",
        "        tgt  = cdf[target_col].values\n",
        "        if len(cdf) < seq_len + 1:\n",
        "            continue\n",
        "        for i in range(len(cdf) - seq_len):\n",
        "            xt = vals[i:i+seq_len, :]\n",
        "            yt = tgt[i+seq_len]\n",
        "            if not np.isnan(yt):\n",
        "                X_list.append(xt)\n",
        "                y_list.append(yt)\n",
        "    if len(X_list)==0:\n",
        "        return np.empty((0,seq_len,len(feature_cols))), np.array([])\n",
        "    X = np.stack(X_list)\n",
        "    y = np.array(y_list)\n",
        "    return X, y\n",
        "\n",
        "def build_lstm(trial, seq_len: int, n_features: int):\n",
        "    units = trial.suggest_int('units', 32, 128)\n",
        "    layers = trial.suggest_int('layers', 1, 3)\n",
        "    dropout = trial.suggest_float('dropout', 0.1, 0.5)\n",
        "    lr = trial.suggest_float('lr', 1e-4, 1e-2, log=True)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units=units, return_sequences=(layers>1), input_shape=(seq_len, n_features)))\n",
        "    model.add(Dropout(dropout))\n",
        "    for i in range(layers-1):\n",
        "        model.add(LSTM(units=units, return_sequences=(i < layers-2)))\n",
        "        model.add(Dropout(dropout))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr), loss='mse')\n",
        "    return model\n",
        "\n",
        "def fit_target_model(panel, feature_cols, target_col, seq_len=SEQ_LEN):\n",
        "    # scale features globally (minmax)\n",
        "    scaler = MinMaxScaler()\n",
        "    scaled = panel.copy()\n",
        "    scaled[feature_cols] = scaler.fit_transform(panel[feature_cols].astype(float))\n",
        "\n",
        "    # build sequences\n",
        "    X, y = build_sequences(scaled, seq_len, feature_cols, target_col)\n",
        "    if X.size == 0 or y.size == 0:\n",
        "        print(f\"[WARN] No training sequences for target '{target_col}'. Skipping model.\")\n",
        "        return None, scaler\n",
        "\n",
        "    #Train, test split\n",
        "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE)\n",
        "\n",
        "    def objective(trial):\n",
        "        model = build_lstm(trial, seq_len, len(feature_cols))\n",
        "        model.fit(Xtr, ytr, epochs=EPOCHS_TUNE, batch_size=BATCH_SIZE, validation_split=0.2, verbose=0)\n",
        "        loss = model.evaluate(Xte, yte, verbose=0)\n",
        "        return loss\n",
        "\n",
        "    study = optuna.create_study(direction='minimize')\n",
        "    study.optimize(objective, n_trials=N_TRIALS)\n",
        "    best = study.best_trial.params\n",
        "\n",
        "    # final model\n",
        "    class _FT(optuna.trial.Trial):\n",
        "        def __init__(self, params): self.params = params\n",
        "        def suggest_int(self, k,a,b): return self.params[k]\n",
        "        def suggest_float(self, k,a,b,log=False): return self.params[k]\n",
        "    final_model = build_lstm(_FT(best), seq_len, len(feature_cols))\n",
        "    final_model.fit(Xtr, ytr, epochs=EPOCHS_FINAL, batch_size=BATCH_SIZE, validation_split=0.2, verbose=1)\n",
        "    test_loss = final_model.evaluate(Xte, yte, verbose=0)\n",
        "    print(f\"Trained '{target_col}' – test MSE: {test_loss:.4e}, best params: {best}\")\n",
        "    return final_model, scaler\n",
        "\n",
        "models = {}\n",
        "scalers = {}\n",
        "\n",
        "for col, nice in TARGETS.items():\n",
        "    if col in df.columns and (~df[col].isna()).sum() >= 100:  # need some signal\n",
        "        m, s = fit_target_model(df, feature_cols, col, seq_len=SEQ_LEN)\n",
        "        models[col] = m\n",
        "        scalers[col] = s\n",
        "    else:\n",
        "        print(f\"Target '{col}' not found or too sparse. It will be skipped.\")\n",
        "def clamp01(x): return max(0.0, min(1.0, float(x)))\n",
        "\n",
        "def build_future_panel(panel, feature_cols, until_year=2030):\n",
        "    fut_rows = []\n",
        "    for cc, cdf in panel.groupby('Country Code'):\n",
        "        cdf = cdf.sort_values('Year')\n",
        "        last_year = int(cdf['Year'].max())\n",
        "        last = cdf.iloc[-1].copy()\n",
        "        if 'disaster_severity' not in cdf.columns:\n",
        "            cdf['disaster_severity'] = 0.0\n",
        "            last['disaster_severity'] = 0.0\n",
        "        for y in range(last_year+1, until_year+1):\n",
        "            row = last.copy()\n",
        "            row['Year'] = y\n",
        "            fut_rows.append(row)\n",
        "    fut = pd.DataFrame(fut_rows)\n",
        "    full = pd.concat([panel, fut], ignore_index=True)\n",
        "    # Nan filling logic as present in other codes\n",
        "    for c in feature_cols:\n",
        "        full[c] = full.groupby('Country Code')[c].ffill()\n",
        "        full[c] = full[c].fillna(full[c].median())\n",
        "    return full\n",
        "\n",
        "def apply_disaster_2026(future_df):\n",
        "    if 'disaster_severity' in future_df.columns:\n",
        "        future_df.loc[future_df['Year'] == 2026, 'disaster_severity'] = np.maximum(\n",
        "            future_df.loc[future_df['Year'] == 2026, 'disaster_severity'].fillna(0), 9.0)\n",
        "    if 'gdp_growth_pct' in future_df.columns:\n",
        "        future_df.loc[future_df['Year'] >= 2026, 'gdp_growth_pct'] *= 0.9 # For small dagging\n",
        "    return future_df\n",
        "\n",
        "def apply_trade_war_2027(future_df):\n",
        "    #-20% trade volume proxy\n",
        "    for c in ['trade_gdp_pct','imports_goods_services_gdp_pct','exports_goods_services_gdp_pct']:\n",
        "        if c in future_df.columns:\n",
        "            future_df.loc[future_df['Year'] >= 2027, c] *= 0.8\n",
        "    #increase concentration / reduce diversification\n",
        "    if 'Trade_Diversification_Index' in future_df.columns:\n",
        "        future_df.loc[future_df['Year'] >= 2027, 'Trade_Diversification_Index'] *= 0.9\n",
        "    for hh in ['HHI_export','HHI_import']:\n",
        "        if hh in future_df.columns:\n",
        "            future_df.loc[future_df['Year'] >= 2027, hh] *= 1.1\n",
        "    return future_df\n",
        "\n",
        "def apply_best_case_2030(future_df):\n",
        "  #LOwers the concentaration for poor performance\n",
        "    if 'Trade_Diversification_Index' in future_df.columns:\n",
        "        future_df.loc[future_df['Year'] == 2030, 'Trade_Diversification_Index'] *= 1.15\n",
        "    for hh in ['HHI_export','HHI_import']:\n",
        "        if hh in future_df.columns:\n",
        "            future_df.loc[future_df['Year'] == 2030, hh] *= 0.85\n",
        "    if 'Current account balance (% of GDP)' in future_df.columns:\n",
        "        future_df.loc[future_df['Year'] == 2030, 'Current account balance (% of GDP)'] *= 1.10\n",
        "    if 'disaster_severity' in future_df.columns:\n",
        "        future_df.loc[future_df['Year'] == 2030, 'disaster_severity'] = 0.0\n",
        "    return future_df\n",
        "\n",
        "def apply_worst_case_2030(future_df):\n",
        "    # Recurring disasters + concentration\n",
        "    if 'disaster_severity' in future_df.columns:\n",
        "        future_df.loc[future_df['Year'].between(2026,2030), 'disaster_severity'] = 8.5\n",
        "    if 'Trade_Diversification_Index' in future_df.columns:\n",
        "        future_df.loc[future_df['Year'] == 2030, 'Trade_Diversification_Index'] *= 0.85\n",
        "    for hh in ['HHI_export','HHI_import']:\n",
        "        if hh in future_df.columns:\n",
        "            future_df.loc[future_df['Year'] == 2030, hh] *= 1.15\n",
        "    return future_df\n",
        "\n",
        "def predict_2030(panel_base, models, scalers, feature_cols, seq_len=SEQ_LEN):\n",
        "    out = {}\n",
        "    for tgt, model in models.items():\n",
        "        if model is None:\n",
        "            continue\n",
        "        res = []\n",
        "        # scaling features with training scaler\n",
        "        scl = scalers[tgt]\n",
        "        scaled = panel_base.copy()\n",
        "        scaled[feature_cols] = scl.transform(scaled[feature_cols].astype(float))\n",
        "        for cc, cdf in scaled.groupby('Country Code'):\n",
        "            cdf = cdf.sort_values('Year')\n",
        "            # build last sequence ending at 2030-1 (years t-4..t for seq_len=5)\n",
        "            hist = cdf[cdf['Year'] <= 2030]\n",
        "            if len(hist) < seq_len:\n",
        "                continue\n",
        "            seq = hist[feature_cols].iloc[-seq_len:].values\n",
        "            seq = np.expand_dims(seq, 0)\n",
        "            pred = float(model.predict(seq, verbose=0).ravel()[0])\n",
        "            res.append({'Country Code': cc, 'Year': 2030, f'pred_{tgt}': pred})\n",
        "        out[tgt] = pd.DataFrame(res)\n",
        "    return out\n",
        "\n",
        "#Future forcasting\n",
        "future_base = build_future_panel(df, feature_cols, until_year=2030)\n",
        "\n",
        "#Q10a\n",
        "future_disaster = apply_disaster_2026(future_base.copy())\n",
        "pred_disaster = predict_2030(future_disaster, models, scalers, feature_cols)\n",
        "\n",
        "#Q10b\n",
        "future_tradewar = apply_trade_war_2027(future_base.copy())\n",
        "pred_tradewar = predict_2030(future_tradewar, models, scalers, feature_cols)\n",
        "\n",
        "#Q11a\n",
        "future_best = apply_best_case_2030(future_base.copy())\n",
        "pred_best = predict_2030(future_best, models, scalers, feature_cols)\n",
        "\n",
        "#Q11b\n",
        "future_worst = apply_worst_case_2030(future_base.copy())\n",
        "pred_worst = predict_2030(future_worst, models, scalers, feature_cols)\n",
        "\n",
        "#For Q10/11 collating teh outpts\n",
        "def collect_outputs(label, dict_preds):\n",
        "    # merge across targets if multiple were trained\n",
        "    keys = list(dict_preds.keys())\n",
        "    if not keys:\n",
        "        return pd.DataFrame(columns=['Country Code','Year', 'Scenario'])\n",
        "    merged = None\n",
        "    for k in keys:\n",
        "        dfk = dict_preds[k].copy()\n",
        "        if merged is None: merged = dfk\n",
        "        else: merged = merged.merge(dfk, on=['Country Code','Year'], how='outer')\n",
        "    merged['Scenario'] = label\n",
        "    return merged\n",
        "\n",
        "q10a_df = collect_outputs('Disaster_2026', pred_disaster)\n",
        "q10b_df = collect_outputs('TradeWar_2027', pred_tradewar)\n",
        "q11a_df = collect_outputs('BestCase_2030',  pred_best)\n",
        "q11b_df = collect_outputs('WorstCase_2030', pred_worst)\n",
        "\n",
        "#Mapping with country names\n",
        "if 'Country Name' in master_df.columns:\n",
        "    for d in [q10a_df, q10b_df, q11a_df, q11b_df]:\n",
        "        d['Country Name'] = d['Country Code'].map(master_df.set_index('Country Code')['Country Name'])\n",
        "\n",
        "q10a_df.to_csv('/content/Q10a_disaster_2026_predictions_2030.csv', index=False)\n",
        "q10b_df.to_csv('/content/Q10b_tradewar_2027_predictions_2030.csv', index=False)\n",
        "q11a_df.to_csv('/content/Q11a_bestcase_2030_predictions.csv', index=False)\n",
        "q11b_df.to_csv('/content/Q11b_worstcase_2030_predictions.csv', index=False)\n",
        "\n",
        "print(\"Saved Q10/Q11 scenario outputs.\")\n",
        "# Q12\n",
        "RES_MAP = {\n",
        "    'Current account balance (% of GDP)': +1,\n",
        "}\n",
        "\n",
        "def zscore_by_year(df_, col):\n",
        "    df_ = df_.copy()\n",
        "    df_['__z__'] = df_.groupby('Year')[col].transform(\n",
        "        lambda v: (v - v.mean())/v.std(ddof=0) if v.std(ddof=0) else 0.0\n",
        "    )\n",
        "    return df_['__z__']\n",
        "\n",
        "def minmax_by_year(df_, col):\n",
        "    df_ = df_.copy()\n",
        "    vmin = df_.groupby('Year')[col].transform('min')\n",
        "    vmax = df_.groupby('Year')[col].transform('max')\n",
        "    out = (df_[col] - vmin) / (vmax - vmin)\n",
        "    out[(vmax - vmin)==0] = 0.5\n",
        "    return out\n",
        "\n",
        "# Start from best-case future (investments)\n",
        "res_2030_base = future_best[future_best['Year'] == 2030].copy()\n",
        "if res_2030_base.empty:\n",
        "    print(\"[WARN] No rows for 2030 in future panel; cannot compute resilience rankings.\")\n",
        "    top5 = pd.DataFrame(columns=['Country Code','Country Name','Resilience_Composite_2030'])\n",
        "else:\n",
        "    comp_list = []\n",
        "    for series, direction in RES_MAP.items():\n",
        "        if series in res_2030_base.columns:\n",
        "            tmp = res_2030_base[['Country Code','Country Name','Year', series]].copy()\n",
        "            tmp['z'] = zscore_by_year(tmp.rename(columns={series:'val'}), 'val') * direction\n",
        "            comp_list.append(tmp[['Country Code','Year','z']].rename(columns={'z':f'Z_{series}'}))\n",
        "    if comp_list:\n",
        "        comp = comp_list[0]\n",
        "        for c in comp_list[1:]:\n",
        "            comp = comp.merge(c, on=['Country Code','Year'], how='outer')\n",
        "        # average Z, then minmax to 0..1\n",
        "        z_cols = [c for c in comp.columns if c.startswith('Z_')]\n",
        "        comp['Z_mean'] = comp[z_cols].mean(axis=1)\n",
        "        comp = comp.merge(res_2030_base[['Country Code','Country Name','Year']], on=['Country Code','Year'], how='left').drop_duplicates()\n",
        "        comp['Resilience_Composite_2030'] = minmax_by_year(comp, 'Z_mean')\n",
        "        comp = comp.sort_values('Resilience_Composite_2030', ascending=False)\n",
        "        top5 = comp[['Country Code','Country Name','Resilience_Composite_2030']].head(5)\n",
        "    else:\n",
        "        print(\"No resilience series available to compute composite.\")\n",
        "        top5 = pd.DataFrame(columns=['Country Code','Country Name','Resilience_Composite_2030'])\n",
        "\n",
        "#show which features most improved under best-case vs worst-case in 2030 for these top countries\n",
        "drivers = []\n",
        "if not top5.empty:\n",
        "    best_2030 = future_best[future_best['Year']==2030].set_index('Country Code')\n",
        "    worst_2030 = future_worst[future_worst['Year']==2030].set_index('Country Code')\n",
        "    for cc in top5['Country Code']:\n",
        "        row = {'Country Code': cc}\n",
        "        for f in ['Trade_Diversification_Index','HHI_export','HHI_import','Current account balance (% of GDP)','trade_gdp_pct','disaster_severity']:\n",
        "            if f in best_2030.columns and f in worst_2030.columns and cc in best_2030.index and cc in worst_2030.index:\n",
        "                try:\n",
        "                    diff = float(best_2030.loc[cc,f]) - float(worst_2030.loc[cc,f])\n",
        "                    row[f'driver_{f}'] = diff\n",
        "                except Exception:\n",
        "                    pass\n",
        "        drivers.append(row)\n",
        "drivers_df = pd.DataFrame(drivers)\n",
        "\n",
        "# Save Q12\n",
        "top5.to_csv('/content/Q12_top5_resilience_2030.csv', index=False)\n",
        "drivers_df.to_csv('/content/Q12_top5_resilience_drivers.csv', index=False)\n",
        "\n",
        "print(\"Saved Q12 resilience rankings and drivers.\")\n",
        "def pretty_targets(cols):\n",
        "    return [TARGETS.get(c, c) for c in cols]\n",
        "\n",
        "print(\"\\n=== Q10: 2030 predictions under shocks ===\")\n",
        "print(\"Disaster 2026 (head):\")\n",
        "print(q10a_df.head(10))\n",
        "print(\"\\nTrade War 2027 (head):\")\n",
        "print(q10b_df.head(10))\n",
        "\n",
        "print(\"\\n=== Q11: 2030 GDP & poverty (best vs worst) ===\")\n",
        "print(\"Best Case (head):\")\n",
        "print(q11a_df.head(10))\n",
        "print(\"\\nWorst Case (head):\")\n",
        "print(q11b_df.head(10))\n",
        "\n",
        "print(\"\\n=== Q12: Top-5 resilience tier by 2030 ===\")\n",
        "print(top5)\n",
        "print(\"\\nDrivers (best - worst deltas) for those countries:\")\n",
        "print(drivers_df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "30Rli8dpECYV",
        "outputId": "15455b10-65ea-4a1f-f411-8ea69ea574ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "incompatible index of inserted column with frame index",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_reindex_for_setitem\u001b[0;34m(value, index)\u001b[0m\n\u001b[1;32m  12686\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 12687\u001b[0;31m         \u001b[0mreindexed_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m  12688\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mreindex\u001b[0;34m(self, index, axis, method, copy, level, fill_value, limit, tolerance)\u001b[0m\n\u001b[1;32m   5152\u001b[0m     ) -> Series:\n\u001b[0;32m-> 5153\u001b[0;31m         return super().reindex(\n\u001b[0m\u001b[1;32m   5154\u001b[0m             \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mreindex\u001b[0;34m(self, labels, index, columns, axis, method, copy, level, fill_value, limit, tolerance)\u001b[0m\n\u001b[1;32m   5609\u001b[0m         \u001b[0;31m# perform the reindex on the axes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5610\u001b[0;31m         return self._reindex_axes(\n\u001b[0m\u001b[1;32m   5611\u001b[0m             \u001b[0maxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_reindex_axes\u001b[0;34m(self, axes, level, limit, tolerance, method, fill_value, copy)\u001b[0m\n\u001b[1;32m   5632\u001b[0m             \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5633\u001b[0;31m             new_index, indexer = ax.reindex(\n\u001b[0m\u001b[1;32m   5634\u001b[0m                 \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mreindex\u001b[0;34m(self, target, method, level, limit, tolerance)\u001b[0m\n\u001b[1;32m   4432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4433\u001b[0;31m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrap_reindex_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreserve_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4434\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/multi.py\u001b[0m in \u001b[0;36m_wrap_reindex_result\u001b[0;34m(self, target, indexer, preserve_names)\u001b[0m\n\u001b[1;32m   2716\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2717\u001b[0;31m                     \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiIndex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tuples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2718\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/multi.py\u001b[0m in \u001b[0;36mnew_meth\u001b[0;34m(self_or_cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_or_cls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/multi.py\u001b[0m in \u001b[0;36mfrom_tuples\u001b[0;34m(cls, tuples, sortorder, names)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 617\u001b[0;31m             \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtuples_to_object_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mlib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.tuples_to_object_array\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Buffer dtype mismatch, expected 'Python object' but got 'long'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3843029321.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;31m# fill per-country median, then global median\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m         \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Country Code'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4309\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4310\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4311\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4313\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4522\u001b[0m         \u001b[0mensure\u001b[0m \u001b[0mhomogeneity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4523\u001b[0m         \"\"\"\n\u001b[0;32m-> 4524\u001b[0;31m         \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4526\u001b[0m         if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   5261\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5262\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5263\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_reindex_for_setitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_reindex_for_setitem\u001b[0;34m(value, index)\u001b[0m\n\u001b[1;32m  12692\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  12693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 12694\u001b[0;31m         raise TypeError(\n\u001b[0m\u001b[1;32m  12695\u001b[0m             \u001b[0;34m\"incompatible index of inserted column with frame index\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  12696\u001b[0m         ) from err\n",
            "\u001b[0;31mTypeError\u001b[0m: incompatible index of inserted column with frame index"
          ]
        }
      ]
    }
  ]
}